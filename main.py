from langchain.chains import RetrievalQA
from langchain.prompts import PromptTemplate
import streamlit as st
from langchain.vectorstores.chroma import Chroma
from langchain.memory import ConversationBufferMemory
from langchain.chains import ConversationalRetrievalChain
import zhipuai

from embedding.call_embedding import get_embedding
from qa_chain.QA_chain_self import QA_chain_self
from qa_chain.Chat_QA_chain_self import Chat_QA_chain_self
import sys
from dotenv import load_dotenv, find_dotenv
import os
from create_db import create_db, load_knowledge_db, create_db_info

##ç•Œé¢
_ = load_dotenv(find_dotenv())
DEFAULT_DB_PATH = "/knowledge_db" # çŸ¥è¯†åº“æ–‡ä»¶æ‰€åœ¨è·¯å¾„
DEFAULT_PERSIST_PATH = "/vector_db"     # å‘é‡æ•°æ®åº“æŒä¹…åŒ–è·¯å¾„
sys.path.append("/knowledge_db")
sys.path.append("/vector_db")
######ç•Œé¢è®¾è®¡#########

from qa_chain.model_to_llm import model_to_llm


def main():
    st.set_page_config(page_title="ChatGPT Assistant", layout="wide")
    # å®šä¹‰çŸ¥è¯†åº“æ–‡ä»¶æ‰€åœ¨è·¯å¾„å’Œå‘é‡æ•°æ®åº“æŒä¹…åŒ–è·¯å¾„
    answer = None
    st.session_state.persist_path = DEFAULT_PERSIST_PATH
    st.session_state.file_path = DEFAULT_DB_PATH
    # å®šä¹‰èŠå¤©æ¨¡å¼é€‰é¡¹
    modes = ["None", "qa_chain", "chat_qa_chain"]
    mode_captions = [
        "ä¸ä½¿ç”¨æ£€ç´¢é—®ç­”çš„æ™®é€šæ¨¡å¼",
        "ä¸å¸¦å†å²è®°å½•çš„æ£€ç´¢é—®ç­”æ¨¡å¼",
        "å¸¦å†å²è®°å½•çš„æ£€ç´¢é—®ç­”æ¨¡å¼"
    ]

    # åˆ›å»ºæ ‡ç­¾é¡µ
    tabs = st.tabs(["ğŸ’¬ èŠå¤©", "ğŸ—’ï¸ é¢„è®¾", "âš™ï¸ æ¨¡å‹", "ğŸ› ï¸ åŠŸèƒ½"])
    from qa_chain.Chat_QA_chain_self import Chat_QA_chain_self

    # èŠå¤©æ ‡ç­¾é¡µå†…å®¹
    with tabs[0]:  # ä½¿ç”¨ç´¢å¼•0æ¥å¼•ç”¨ç¬¬ä¸€ä¸ªæ ‡ç­¾é¡µï¼Œå³"ğŸ’¬ èŠå¤©"
        st.header("èŠå¤©ç•Œé¢")

        # æ£€æŸ¥ä¼šè¯çŠ¶æ€ä¸­æ˜¯å¦å·²ç»æœ‰æ¶ˆæ¯åˆ—è¡¨ï¼Œå¦‚æœæ²¡æœ‰åˆ™åˆå§‹åŒ–
        if 'messages' not in st.session_state:
            st.session_state['messages'] = []

        # é€‰æ‹©å¯¹è¯æ¨¡å¼
        selected_method = st.radio(
            "ä½ æƒ³é€‰æ‹©å“ªç§æ¨¡å¼è¿›è¡Œå¯¹è¯ï¼Ÿ",
            options=modes,
            captions=mode_captions,
            key="selected_method"
        )

        # èŠå¤©è¾“å…¥æ¡†
        with st.form("chat_form"):
            prompt = st.text_input("Say something:", key="prompt")
            submitted = st.form_submit_button("Submit")

        # å¦‚æœæäº¤äº†èŠå¤©è¾“å…¥
        if submitted and prompt:
            if selected_method == "None":
                st.session_state['messages'].append({"role": "user", "text": prompt})
                # è°ƒç”¨ model_to_llm å‡½æ•°è·å– LLM å®ä¾‹
                llm = model_to_llm(model=st.session_state.selected_model, temperature=st.session_state.temperature,
                                   api_key=st.session_state.temperature)
                # ä½¿ç”¨ LLM å®ä¾‹ç”Ÿæˆå›ç­”
                # ä½¿ç”¨get_completionç”Ÿæˆanswer
                answer = llm.get_completion(prompt, llm, temperature=st.session_state.temperature,
                                            max_tokens=st.session_state.temperature)

                # å°†LLMçš„å›ç­”æ·»åŠ åˆ°å¯¹è¯å†å²ä¸­
            elif selected_method == "qa_chain":  # ä¸å¸¦å†å²è®°å½•çš„æ£€ç´¢é—®ç­”æ¨¡å¼
                if 'qa_chain' not in st.session_state:
                    # åˆå§‹åŒ– QA_chain_self å®ä¾‹
                    QA = QA_chain_self(
                        model=st.session_state.selected_model,
                        temperature=st.session_state.temperature,
                        top_k=3,
                        file_path=st.session_state.file_path,
                        persist_path=st.session_state.persist_path,
                        api_key=st.session_state.llm_api_key,
                        embedding=st.session_state.embeddings_choice,
                        embedding_key=st.session_state.llm_api_key
                    )
                    answer = QA.answer(question=prompt)
            elif selected_method == "chat_qa_chain":  # å¸¦å†å²è®°å½•çš„æ£€ç´¢é—®ç­”æ¨¡å¼
                if 'chat_qa_chain' not in st.session_state:
                    chat = Chat_QA_chain_self(
                        model=st.session_state.selected_model,
                        temperature=st.session_state.temperature,
                        top_k=3,
                        file_path=st.session_state.file_path,
                        persist_path=st.session_state.persist_path,
                        api_key=st.session_state.llm_api_key,
                        embedding=st.session_state.embeddings_choice,
                        embedding_key=st.session_state.llm_api_key
                    )
                    answer = chat.answer(question=prompt)
        if answer is not None:
            # å°†LLMçš„å›ç­”æ·»åŠ åˆ°å¯¹è¯å†å²ä¸­
            st.session_state.messages.append({"role": "assistant", "text": answer})
    # æ˜¾ç¤ºæ•´ä¸ªå¯¹è¯å†å²
    for message in st.session_state['messages']:
        if message["role"] == "user":
            st.write(f"ç”¨æˆ·: {message['text']}")
        elif message["role"] == "assistant":
            st.write(f"åŠ©æ‰‹: {message['text']}")


    with tabs[1]:
        st.header("é¢„è®¾")

    with tabs[2]:
        LLM_MODEL_DICT = {
            "openai": ["gpt-3.5-turbo", "gpt-3.5-turbo-16k-0613", "gpt-3.5-turbo-0613", "gpt-4", "gpt-4-32k"],
            "wenxin": ["ERNIE-Bot", "ERNIE-Bot-4", "ERNIE-Bot-turbo"],
            "xinhuo": ["Spark-1.5", "Spark-2.0"],
            "zhipuai": ["chatglm_pro", "chatglm_std", "chatglm_lite"]
        }

        st.header("æ¨¡å‹è®¾ç½®")

        # åˆå§‹åŒ–æ¸©åº¦æ»‘å—
        if "temperature" not in st.session_state:
            st.session_state.temperature = 0.7
        st.session_state.temperature = st.slider("Temperature", 0.0, 1.0, st.session_state.temperature)

        # åˆå§‹åŒ–å¹³å°é€‰æ‹©
        if "selected_platform" not in st.session_state:
            st.session_state.selected_platform = "openai"

        # å¹³å°é€‰æ‹©
        selected_platform = st.selectbox(
            "é€‰æ‹©å¹³å°ï¼š",
            options=list(LLM_MODEL_DICT.keys()),
            index=list(LLM_MODEL_DICT.keys()).index(st.session_state.selected_platform)
        )

        # å½“é€‰æ‹©çš„ platform å‘ç”Ÿå˜åŒ–æ—¶ï¼Œé‡æ–°åˆå§‹åŒ– selected_model
        if selected_platform != st.session_state.selected_platform:
            st.session_state.selected_platform = selected_platform
            st.session_state.selected_model = LLM_MODEL_DICT[selected_platform][0]  # æ›´æ–°ä¸ºæ–°å¹³å°çš„é»˜è®¤æ¨¡å‹

        # åˆå§‹åŒ–æ¨¡å‹é€‰æ‹©
        if "selected_model" not in st.session_state:
            st.session_state.selected_model = LLM_MODEL_DICT[selected_platform][0]

        # æ¨¡å‹é€‰æ‹©
        selected_model = st.selectbox(
            "é€‰æ‹©æ¨¡å‹ï¼š",
            options=LLM_MODEL_DICT[selected_platform],
            index=LLM_MODEL_DICT[selected_platform].index(st.session_state.selected_model),
            key="selected_model"
        )

        # æ›´æ–° selected_model åˆ° session_state
        if selected_model != st.session_state.selected_model:
            st.session_state.selected_model = selected_model

        # LLM APIå¯†é’¥è¾“å…¥
        api_key = st.text_input("LLM API Key", type="password")

        # æ˜¾ç¤ºå½“å‰é€‰æ‹©
        st.write(f"å½“å‰é€‰æ‹©çš„å¹³å°ï¼š{st.session_state.selected_platform}")
        st.write(f"å½“å‰é€‰æ‹©çš„æ¨¡å‹ï¼š{st.session_state.selected_model}")

        # ä¿å­˜ API å¯†é’¥
        if api_key:
            st.session_state.llm_api_key = api_key
            st.success("APIå¯†é’¥å·²ä¿å­˜ã€‚")
        else:
            st.warning("è¯·æä¾›LLM APIå¯†é’¥ä»¥ä½¿ç”¨æ¨¡å‹åŠŸèƒ½ã€‚")

    with tabs[3]:
        st.header("åŠŸèƒ½è®¾ç½®")

        # æ¸…é™¤èŠå¤©è®°å½•çš„æŒ‰é’®
        clear = st.button("æ¸…é™¤èŠå¤©è®°å½•")

        # å¦‚æœç”¨æˆ·ç‚¹å‡»äº†æ¸…é™¤èŠå¤©è®°å½•æŒ‰é’®
        if clear:
            # æ£€æŸ¥ä¼šè¯çŠ¶æ€ä¸­æ˜¯å¦æœ‰èŠå¤©è®°å½•ï¼Œå¦‚æœæœ‰åˆ™æ¸…é™¤
            if 'messages' in st.session_state:
                st.session_state.messages = []
                st.success("èŠå¤©è®°å½•å·²æ¸…é™¤ï¼")
            else:
                st.info("æ²¡æœ‰èŠå¤©è®°å½•å¯æ¸…é™¤ã€‚")



    with st.sidebar:
        st.header("çŸ¥è¯†åº“è®¾ç½®")

        # ä¸Šä¼ æ–‡ä»¶
        uploaded_file = st.file_uploader("ä¸Šä¼ çŸ¥è¯†åº“æ–‡ä»¶", type=["txt", "pdf", "md", "docx"])

        if uploaded_file is not None:
                # æ„å»ºæ–‡ä»¶è·¯å¾„ï¼Œç¡®ä¿æ–‡ä»¶åæ²¡æœ‰å†²çª
            if not os.path.exists(DEFAULT_DB_PATH):
                os.makedirs(DEFAULT_DB_PATH)  # å¦‚æœæ–‡ä»¶å¤¹ä¸å­˜åœ¨ï¼Œåˆ™åˆ›å»ºæ–‡ä»¶å¤¹

            file_path = os.path.join(DEFAULT_DB_PATH, uploaded_file.name)
            file_bytes = uploaded_file.read()
                # ä¿å­˜ä¸Šä¼ çš„æ–‡ä»¶

            with open(file_path, 'wb') as f:
                f.write(file_bytes)
            st.success(f"æ–‡ä»¶ {uploaded_file.name} å·²æˆåŠŸä¸Šä¼ ï¼")

        st.session_state.embeddings_choice = st.selectbox("é€‰æ‹© Embedding æ¨¡å‹", ["openai", "m3e", "zhipuai"])


if __name__ == "__main__":
    main()
